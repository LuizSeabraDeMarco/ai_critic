Metadata-Version: 2.4
Name: ai-critic
Version: 0.2.0
Summary: Fast AI evaluator for scikit-learn models
Author-email: Luiz Seabra <filipedemarco@yahoo.com>
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: numpy
Requires-Dist: scikit-learn

# ai-critic

Automated Risk Auditor for Machine Learning Models

[![PyPI version](https://img.shields.io/pypi/v/ai-critic.svg)](https://pypi.org/project/ai-critic/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸš€ O que Ã© ai-critic?

**ai-critic** Ã© um auditor de risco automatizado e baseado em heurÃ­sticas para modelos de *machine learning*. Ele avalia modelos treinados antes da implantaÃ§Ã£o e traduz riscos tÃ©cnicos de ML em decisÃµes claras e centradas no ser humano.

Em vez de apenas relatar mÃ©tricas, **ai-critic** responde a uma pergunta crÃ­tica:

> â€œEste modelo pode ser implantado com seguranÃ§a?â€

Ele faz isso analisando:
*   **Integridade dos Dados:** (vazamento de dados, desequilÃ­brio, valores NaNs)
*   **Estrutura do Modelo:** (risco de *overfitting*, complexidade)
*   **Comportamento de ValidaÃ§Ã£o:** (pontuaÃ§Ãµes suspeitamente perfeitas)
*   **Robustez:** (sensibilidade a ruÃ­do)

Os resultados sÃ£o organizados em trÃªs camadas semÃ¢nticas para diferentes *stakeholders*:
1.  **Executiva:** (tomadores de decisÃ£o)
2.  **TÃ©cnica:** (engenheiros de ML)
3.  **Detalhes:** (auditores e depuraÃ§Ã£o)

## ğŸ¯ Por que ai-critic existe: Filosofia Central

A maioria das ferramentas de ML:
*   assume que mÃ©tricas = verdade
*   confia cegamente na validaÃ§Ã£o cruzada
*   expÃµe nÃºmeros brutos sem interpretaÃ§Ã£o

**ai-critic Ã© cÃ©tico por design.**

Ele trata:
*   pontuaÃ§Ãµes perfeitas como sinais, nÃ£o como sucesso
*   mÃ©tricas de robustez como dependentes do contexto
*   a implantaÃ§Ã£o como uma decisÃ£o de risco, nÃ£o um limite de mÃ©trica

A filosofia central Ã©: **MÃ©tricas nÃ£o falham modelos â€” o contexto falha.**

ai-critic aplica heurÃ­sticas de raciocÃ­nio humano Ã  avaliaÃ§Ã£o de *machine learning*:
*   â€œIsso Ã© bom demais para ser verdade?â€
*   â€œIsso pode estar vazando o alvo?â€
*   â€œA robustez ainda importa se a linha de base estiver errada?â€

## ğŸ› ï¸ InstalaÃ§Ã£o

VocÃª pode instalar `ai-critic` usando pip:

```bash
pip install ai-critic
```

**Requisitos:**
*   Python â‰¥ 3.8
*   scikit-learn

## ğŸ’¡ InÃ­cio RÃ¡pido

Audite seu modelo treinado em poucas linhas de cÃ³digo:

```python
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from ai_critic import AICritic

# 1. Carregar dados e treinar um modelo (exemplo)
X, y = load_breast_cancer(return_X_y=True)
model = RandomForestClassifier(max_depth=20, random_state=42)
model.fit(X, y) # O modelo precisa estar treinado

# 2. Inicializar e avaliar com ai-critic
critic = AICritic(model, X, y)
report = critic.evaluate()

# O padrÃ£o Ã© a visualizaÃ§Ã£o 'all' (todas as camadas)
print(report)
```

## ğŸ§© SaÃ­da Multi-Camadas

`ai-critic` nunca despeja tudo de uma vez. Ele estrutura os resultados em camadas de decisÃ£o claras.

### ğŸ”¹ VisÃ£o Executiva (`view="executive"`)

Projetada para CTOs, gerentes e *stakeholders*. Zero jargÃ£o de ML.

```python
critic.evaluate(view="executive")
```

**Exemplo de SaÃ­da:**
```json
{
  "verdict": "âŒ NÃ£o ConfiÃ¡vel",
  "risk_level": "high",
  "deploy_recommended": false,
  "main_reason": "Forte evidÃªncia de vazamento de dados inflando o desempenho do modelo."
}
```

### ğŸ”¹ VisÃ£o TÃ©cnica (`view="technical"`)

Projetada para engenheiros de ML. Ã‰ acionÃ¡vel, diagnÃ³stica e focada no que precisa ser corrigido.

```python
critic.evaluate(view="technical")
```

**Exemplo de SaÃ­da:**
```json
{
  "key_risks": [
    "Vazamento de dados suspeito devido Ã  correlaÃ§Ã£o quase perfeita entre recurso e alvo.",
    "PontuaÃ§Ã£o de validaÃ§Ã£o cruzada perfeita detectada (estatisticamente improvÃ¡vel).",
    "A profundidade da Ã¡rvore pode ser muito alta para o tamanho do conjunto de dados."
  ],
  "model_health": {
    "data_leakage": true,
    "suspicious_cv": true,
    "structural_risk": true,
    "robustness_verdict": "misleading"
  },
  "recommendations": [
    "Auditar e remover recursos com vazamento.",
    "Reduzir a complexidade do modelo.",
    "Executar novamente a validaÃ§Ã£o apÃ³s a mitigaÃ§Ã£o do vazamento."
  ]
}
```

### ğŸ”¹ VisÃ£o Detalhada (`view="details"`)

Projetada para auditoria, depuraÃ§Ã£o e conformidade.

```python
critic.evaluate(view="details")
```

Inclui:
*   MÃ©tricas brutas
*   CorrelaÃ§Ãµes
*   PontuaÃ§Ãµes de robustez
*   Avisos estruturais
*   Rastreabilidade completa

### ğŸ”¹ VisÃ£o Combinada (`view="all"`)

Retorna todas as trÃªs camadas em um Ãºnico dicionÃ¡rio.

```python
critic.evaluate(view="all")
```

**Retorna:**
```json
{
  "executive": {...},
  "technical": {...},
  "details": {...}
}
```

## âš™ï¸ API Principal

### `AICritic`

| ParÃ¢metro | DescriÃ§Ã£o |
| :--- | :--- |
| `model` | Modelo `scikit-learn` treinado. |
| `X` | Matriz de recursos (features). |
| `y` | Vetor alvo (target). |

**Uso:** `AICritic(model, X, y)`

### `evaluate()`

| ParÃ¢metro | DescriÃ§Ã£o |
| :--- | :--- |
| `view` | A camada de saÃ­da desejada: `"executive"`, `"technical"`, `"details"`, ou `"all"` (padrÃ£o). |

**Uso:** `evaluate(view="all")`

## ğŸ§  O que ai-critic Detecta

| Categoria | Riscos Detectados |
| :--- | :--- |
| **ğŸ” Riscos de Dados** | Vazamento de alvo via correlaÃ§Ã£o, NaNs, DesequilÃ­brio de classe. |
| **ğŸ§± Riscos Estruturais** | Ãrvores excessivamente complexas, Altas proporÃ§Ãµes de recurso/amostra, *Configuration smells*. |
| **ğŸ“ˆ Riscos de ValidaÃ§Ã£o** | PontuaÃ§Ãµes de CV suspeitamente perfeitas, VariÃ¢ncia irrealista. |
| **ğŸ§ª Riscos de Robustez** | Sensibilidade a ruÃ­do, Robustez enganosa quando a linha de base estÃ¡ inflada. |

### ğŸ§ª Exemplo: DetecÃ§Ã£o de Vazamento de Dados

```python
import numpy as np
# ... (cÃ³digo de importaÃ§Ã£o e modelo)

# Vazamento artificial: adicionando o alvo como um recurso
X_leaky = np.hstack([X, y.reshape(-1, 1)])

critic = AICritic(model, X_leaky, y)
executive_report = critic.evaluate(view="executive")

print(executive_report)
```

**Resultado (Executive View):**
```
âŒ NÃ£o ConfiÃ¡vel
Forte evidÃªncia de vazamento de dados inflando o desempenho do modelo.
```

## ğŸ›¡ï¸ Melhores PrÃ¡ticas

*   Execute `ai-critic` antes da implantaÃ§Ã£o.
*   Nunca confie cegamente em pontuaÃ§Ãµes de CV perfeitas.
*   Use a **VisÃ£o Executiva** no seu pipeline de CI/CD como um *gate* de modelo.
*   Use a **VisÃ£o TÃ©cnica** durante a iteraÃ§Ã£o do modelo.
*   Use a **VisÃ£o Detalhada** para auditorias e conformidade.

## ğŸ§­ Casos de Uso TÃ­picos

*   Auditorias de modelo prÃ©-implantaÃ§Ã£o.
*   GovernanÃ§a e conformidade de ML.
*   *Gates* de modelo em CI/CD.
*   Ensino de ceticismo em ML.
*   ExplicaÃ§Ã£o de risco de ML para *stakeholders* nÃ£o tÃ©cnicos.

## ğŸ“„ LicenÃ§a

DistribuÃ­do sob a **LicenÃ§a MIT**.

## ğŸ§  Nota Final

`ai-critic` nÃ£o Ã© uma ferramenta de *benchmark*. **Ã‰ uma ferramenta de decisÃ£o.**

Se um modelo falhar aqui, isso nÃ£o significa que ele Ã© ruim â€” significa que ele nÃ£o deve ser confiÃ¡vel **ainda**.
