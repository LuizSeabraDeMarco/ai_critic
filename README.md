# ai-critic: Automated Risk Auditor for Machine Learning Models**

---

## ğŸš€ What is ai-critic?

`ai-critic` Ã© um **auditor de risco automatizado baseado em heurÃ­sticas** para modelos de *machine learning*. Ele avalia modelos treinados antes da implantaÃ§Ã£o e traduz riscos tÃ©cnicos de ML em decisÃµes claras e centradas no ser humano.

Em vez de apenas relatar mÃ©tricas, o `ai-critic` responde Ã  pergunta crÃ­tica:

> â€œEste modelo pode ser implantado com seguranÃ§a?â€

Ele faz isso analisando as principais Ã¡reas de risco:

*   **Integridade dos Dados:** (*data leakage*, desequilÃ­brio, NaNs)
*   **Estrutura do Modelo:** (risco de *overfitting*, complexidade)
*   **Comportamento de ValidaÃ§Ã£o:** (pontuaÃ§Ãµes suspeitamente perfeitas)
*   **Robustez:** (sensibilidade a ruÃ­do)

Os resultados sÃ£o organizados em trÃªs camadas semÃ¢nticas para diferentes *stakeholders*:

*   **Executiva:** (tomadores de decisÃ£o)
*   **TÃ©cnica:** (engenheiros de ML)
*   **Detalhada:** (auditores e depuraÃ§Ã£o)

## ğŸ¯ Por que o ai-critic Existe: Filosofia Central

A maioria das ferramentas de ML:

*   assume que mÃ©tricas = verdade
*   confia cegamente na validaÃ§Ã£o cruzada
*   despeja nÃºmeros brutos sem interpretaÃ§Ã£o

O `ai-critic` Ã© cÃ©tico por design.

Ele trata:

*   pontuaÃ§Ãµes perfeitas como **sinais**, nÃ£o sucesso
*   mÃ©tricas de robustez como **dependentes do contexto**
*   implantaÃ§Ã£o como uma **decisÃ£o de risco**, nÃ£o um limite de mÃ©trica

A filosofia central Ã©: **MÃ©tricas nÃ£o falham modelos â€” o contexto falha.**

O `ai-critic` aplica heurÃ­sticas de raciocÃ­nio humano Ã  avaliaÃ§Ã£o de ML:

*   â€œIsso Ã© bom demais para ser verdade?â€
*   â€œIsso pode estar vazando o alvo (*target*)?â€
*   â€œA robustez importa se a linha de base estiver errada?â€

## ğŸ› ï¸ InstalaÃ§Ã£o

Instale o `ai-critic` via pip:

```bash
pip install ai-critic
```

**Requisitos:**

*   Python â‰¥ 3.8
*   `scikit-learn`

## ğŸ’¡ InÃ­cio RÃ¡pido

Audite seu modelo treinado em apenas algumas linhas:

```python
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from ai_critic import AICritic

# 1. Carregar dados e treinar um modelo (exemplo)
X, y = load_breast_cancer(return_X_y=True)
model = RandomForestClassifier(max_depth=20, random_state=42)
model.fit(X, y)  # O modelo deve estar treinado

# 2. Inicializar e avaliar com ai-critic
critic = AICritic(model, X, y)
report = critic.evaluate()

# A visualizaÃ§Ã£o padrÃ£o Ã© 'all' (todas as camadas)
print(report)
```

## ğŸ§© SaÃ­da Multi-Camadas

O `ai-critic` nunca despeja tudo de uma vez. Ele estrutura os resultados em camadas de decisÃ£o claras.

### ğŸ”¹ VisualizaÃ§Ã£o Executiva (`view="executive"`)

Projetado para CTOs, gerentes e *stakeholders*. Sem jargÃ£o de ML.

```python
critic.evaluate(view="executive")
```

**Exemplo de SaÃ­da:**

```json
{
  "verdict": "âŒ NÃ£o ConfiÃ¡vel",
  "risk_level": "high",
  "deploy_recommended": false,
  "main_reason": "Forte evidÃªncia de vazamento de dados inflando o desempenho do modelo."
}
```

### ğŸ”¹ VisualizaÃ§Ã£o TÃ©cnica (`view="technical"`)

Projetado para engenheiros de ML. AcionÃ¡vel, diagnÃ³stico e focado no que precisa ser corrigido.

```python
critic.evaluate(view="technical")
```

**Exemplo de SaÃ­da:**

```json
{
  "key_risks": [
    "Vazamento de dados suspeito devido Ã  correlaÃ§Ã£o quase perfeita entre recurso e alvo.",
    "PontuaÃ§Ã£o de validaÃ§Ã£o cruzada perfeita detectada (estatisticamente improvÃ¡vel).",
    "A profundidade da Ã¡rvore pode ser muito alta para o tamanho do conjunto de dados."
  ],
  "model_health": {
    "data_leakage": true,
    "suspicious_cv": true,
    "structural_risk": true,
    "robustness_verdict": "misleading"
  },
  "recommendations": [
    "Auditar e remover recursos com vazamento.",
    "Reduzir a complexidade do modelo.",
    "Executar novamente a validaÃ§Ã£o apÃ³s a mitigaÃ§Ã£o do vazamento."
  ]
}
```

### ğŸ”¹ VisualizaÃ§Ã£o Detalhada (`view="details"`)

Projetado para auditoria, depuraÃ§Ã£o e conformidade.

```python
critic.evaluate(view="details")
```

Inclui:

*   MÃ©tricas brutas
*   CorrelaÃ§Ãµes de recursos
*   PontuaÃ§Ãµes de robustez
*   Avisos estruturais
*   Rastreabilidade completa

### ğŸ”¹ VisualizaÃ§Ã£o Combinada (`view="all"`)

Retorna todas as trÃªs camadas em um Ãºnico dicionÃ¡rio.

```python
critic.evaluate(view="all")
```

**Retorna:**

```json
{
  "executive": {...},
  "technical": {...},
  "details": {...}
}
```

## âš™ï¸ API Principal

### `AICritic`

| ParÃ¢metro | DescriÃ§Ã£o |
| :--- | :--- |
| `model` | Modelo `scikit-learn` treinado |
| `X` | Matriz de recursos |
| `y` | Vetor alvo |

**Uso:** `AICritic(model, X, y)`

### `evaluate()`

| ParÃ¢metro | DescriÃ§Ã£o |
| :--- | :--- |
| `view` | Camada de saÃ­da desejada: `"executive"`, `"technical"`, `"details"`, ou `"all"` (padrÃ£o) |

**Uso:** `evaluate(view="all")`

## ğŸ§  O que o ai-critic Detecta

| Categoria | Riscos Detectados |
| :--- | :--- |
| **ğŸ” Riscos de Dados** | Vazamento de alvo via correlaÃ§Ã£o, NaNs, desequilÃ­brio de classes |
| **ğŸ§± Riscos Estruturais** | Ãrvores excessivamente complexas, altas taxas de recurso/amostra, *configuration smells* |
| **ğŸ“ˆ Riscos de ValidaÃ§Ã£o** | PontuaÃ§Ãµes de CV suspeitosamente perfeitas, variÃ¢ncia irreal |
| **ğŸ§ª Riscos de Robustez** | Sensibilidade a ruÃ­do, robustez enganosa se a linha de base estiver inflada |

## ğŸ§ª Exemplo: Detectando Vazamento de Dados

```python
import numpy as np
# ... (imports e cÃ³digo do modelo)

# Vazamento artificial: adicionando o alvo como um recurso
X_leaky = np.hstack([X, y.reshape(-1, 1)])

critic = AICritic(model, X_leaky, y)
executive_report = critic.evaluate(view="executive")

print(executive_report)
```

**SaÃ­da (VisualizaÃ§Ã£o Executiva):**

```
âŒ NÃ£o ConfiÃ¡vel
Forte evidÃªncia de vazamento de dados inflando o desempenho do modelo.
```

## ğŸ›¡ï¸ Melhores PrÃ¡ticas

*   Execute o `ai-critic` antes da implantaÃ§Ã£o.
*   Nunca confie cegamente em pontuaÃ§Ãµes de CV perfeitas.
*   Use a VisualizaÃ§Ã£o Executiva em seu *pipeline* de CI/CD como um portÃ£o de modelo.
*   Use a VisualizaÃ§Ã£o TÃ©cnica durante a iteraÃ§Ã£o do modelo.
*   Use a VisualizaÃ§Ã£o Detalhada para auditoria e conformidade.

## ğŸ§­ Casos de Uso TÃ­picos

*   Auditorias de modelo prÃ©-implantaÃ§Ã£o
*   GovernanÃ§a e conformidade de ML
*   PortÃµes de modelo CI/CD
*   Ensino de ceticismo em ML
*   ExplicaÃ§Ã£o de risco de ML para *stakeholders* nÃ£o tÃ©cnicos

## ğŸ“„ LicenÃ§a

DistribuÃ­do sob a LicenÃ§a MIT.

## ğŸ§  Nota Final

O `ai-critic` nÃ£o Ã© uma ferramenta de *benchmarking*. Ã‰ uma **ferramenta de decisÃ£o**.

Se um modelo falhar aqui, nÃ£o significa que seja ruim â€” significa que **nÃ£o deve ser confiÃ¡vel ainda**.
