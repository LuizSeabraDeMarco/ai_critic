Performance under noise

> Visualizations are optional and do not affect the decision logic.

---

## âš™ï¸ Main API

### `AICritic(model, X, y)`

* `model`: scikit-learn compatible estimator
* `X`: feature matrix
* `y`: target vector

### `evaluate(view="all", plot=False)`

* `view`: `"executive"`, `"technical"`, `"details"`, `"all"` or custom list
* `plot`: generates graphs when `True`

---

## ğŸ§  What ai-critic Detects

| Category | Risks |

| ------------ | ---------------------------------------- |

| ğŸ” Data | Target Leakage, NaNs, Imbalance |

| ğŸ§± Structure | Excessive Complexity, Overfitting |

| ğŸ“ˆ Validation | Perfect or Statistically Suspicious CV |

| ğŸ§ª Robustness | Stable, Fragile, or Misleading |

---

## ğŸ›¡ï¸ Best Practices

* **CI/CD:** Use executive output as a *quality gate*
* **Iteration:** Use technical output during tuning
* **Governance:** Log detailed output
* **Skepticism:** Never blindly trust a perfect CV

---

## ğŸ§­ Use Cases

* Pre-deployment Audit
* ML Governance
* CI/CD Pipelines
* Risk Communication for Non-Technical Users

---

## ğŸ“„ License

Distributed under the **MIT License**.

---

## ğŸ§  Final Note

**ai-critic** is not a *benchmarking* tool. It's a **decision-making tool**.

If a model fails here, it doesn't mean it's badâ€”it means it **shouldn't be trusted yet**.